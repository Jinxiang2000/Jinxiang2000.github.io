<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Named Entity Recognition with Transformers in PyTorch &#183; Jinxiang Ma</title><meta name=title content="Named Entity Recognition with Transformers in PyTorch &#183; Jinxiang Ma"><meta name=description content="Fine-tuning DistilBERT for NER using the CoNLL-2003 dataset, handling label-token alignment and evaluating span-level accuracy."><meta name=keywords content="“NLP”,“Transformers”,“Named Entity Recognition”,“PyTorch”,“HuggingFace”,"><link rel=canonical href=http://localhost:1313/projects/ner_transformer/><link type=text/css rel=stylesheet href=/css/main.bundle.min.2755fda1868f0bfc1843df512e7b0b0bec55a259db5f8b13dc3c0f7894e519f617c2b66f5915ec207e2a1ffc8b6d1b53dbd8f3dff8892140cb5eacc50a6edfbb.css integrity="sha512-J1X9oYaPC/wYQ99RLnsLC+xVolnbX4sT3DwPeJTlGfYXwrZvWRXsIH4qH/yLbRtT29jz3/iJIUDLXqzFCm7fuw=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.dc15078f1fe553ce354f85ec1bfce7ded8b4060d4124a49b19e31dd64345b8d8f03edbbb1b681748650181f9d81da9728ca10c2b9cb2ae7bf8983d4b669ba876.js integrity="sha512-3BUHjx/lU841T4XsG/zn3ti0Bg1BJKSbGeMd1kNFuNjwPtu7G2gXSGUBgfnYHalyjKEMK5yyrnv4mD1LZpuodg==" data-copy=Copy data-copied=Copied></script><script src=/lib/zoom/zoom.min.f592a181a15d2a5b042daa7f746c3721acf9063f8b6acd175d989129865a37d400ae0e85b640f9ad42cd98d1f8ad30931718cf8811abdcc5fcb264400d1a2b0c.js integrity="sha512-9ZKhgaFdKlsELap/dGw3Iaz5Bj+Las0XXZiRKYZaN9QArg6FtkD5rULNmNH4rTCTFxjPiBGr3MX8smRADRorDA=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="http://localhost:1313/projects/ner_transformer/"><meta property="og:site_name" content="Jinxiang Ma"><meta property="og:title" content="Named Entity Recognition with Transformers in PyTorch"><meta property="og:description" content="Fine-tuning DistilBERT for NER using the CoNLL-2003 dataset, handling label-token alignment and evaluating span-level accuracy."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2025-06-20T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-20T00:00:00+00:00"><meta property="article:tag" content="“NLP”"><meta property="article:tag" content="“Transformers”"><meta property="article:tag" content="“Named Entity Recognition”"><meta property="article:tag" content="“PyTorch”"><meta property="article:tag" content="“HuggingFace”"><meta name=twitter:card content="summary"><meta name=twitter:title content="Named Entity Recognition with Transformers in PyTorch"><meta name=twitter:description content="Fine-tuning DistilBERT for NER using the CoNLL-2003 dataset, handling label-token alignment and evaluating span-level accuracy."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"My Projects","name":"Named Entity Recognition with Transformers in PyTorch","headline":"Named Entity Recognition with Transformers in PyTorch","description":"Fine-tuning DistilBERT for NER using the CoNLL-2003 dataset, handling label-token alignment and evaluating span-level accuracy.","abstract":"\u003ch2 class=\u0022relative group\u0022\u003eIntroduction \n    \u003cdiv id=\u0022introduction\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\u0022\u003e\n        \u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\u0022 href=\u0022#introduction\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e        \n    \n\u003c\/h2\u003e\n\u003cp\u003eNamed Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that involves identifying and classifying entities such as persons, organizations, locations, and miscellaneous names within raw text. In this project, we implemented a Transformer-based NER system using PyTorch and HuggingFace’s Transformers library, focusing on fine-tuning the DistilBERT model on the widely used CoNLL-2003 dataset.\u003c\/p\u003e","inLanguage":"en","url":"http:\/\/localhost:1313\/projects\/ner_transformer\/","author":{"@type":"Person","name":""},"copyrightYear":"2025","dateCreated":"2025-06-20T00:00:00\u002b00:00","datePublished":"2025-06-20T00:00:00\u002b00:00","dateModified":"2025-06-20T00:00:00\u002b00:00","keywords":["“NLP”","“Transformers”","“Named Entity Recognition”","“PyTorch”","“HuggingFace”"],"mainEntityOfPage":"true","wordCount":"706"}]</script><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px] z-index-100"><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3 padding-main-menu"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Jinxiang Ma</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About Me">About</p></a><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="My Projects">Projects</p></a><a href=/skills/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="My Skills">Skills</p></a><a href=/resume/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Resume>Resume</p></a><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Blog</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 padding-top-[5px]"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About Me">About</p></a></li><li class=mt-1><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="My Projects">Projects</p></a></li><li class=mt-1><a href=/skills/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="My Skills">Skills</p></a></li><li class=mt-1><a href=/resume/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Resume>Resume</p></a></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Blog</p></a></li></ul></div></label></div></div></div></div><script type=text/javascript src=/js/background-blur.min.4812321520cc2e4cd52bf9137015a55bb6c0b5e66a6b2c96b8654c0ec99be59e8e6425dd838a6fd9bac54000586f37f4a9680bbefaa2aee4601a69e188005cf6.js integrity="sha512-SBIyFSDMLkzVK/kTcBWlW7bAteZqayyWuGVMDsmb5Z6OZCXdg4pv2brFQABYbzf0qWgLvvqiruRgGmnhiABc9g==" data-target-id=menu-blur></script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Named Entity Recognition with Transformers in PyTorch</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-06-20T00:00:00+00:00>20 June 2025</time><span class="px-2 text-primary-500">&#183;</span><span>706 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">4 mins</span></div></div><div class="flex author"><div class=place-self-center><div class="text-2xl sm:text-lg"></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#dataset-and-preprocessing>Dataset and Preprocessing</a></li><li><a href=#model-architecture>Model Architecture</a></li><li><a href=#training>Training</a></li><li><a href=#evaluation>Evaluation</a></li><li><a href=#results>Results</a></li><li><a href=#lessons-learned>Lessons Learned</a></li><li><a href=#future-work>Future Work</a></li><li><a href=#code-snippets>Code Snippets</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#dataset-and-preprocessing>Dataset and Preprocessing</a></li><li><a href=#model-architecture>Model Architecture</a></li><li><a href=#training>Training</a></li><li><a href=#evaluation>Evaluation</a></li><li><a href=#results>Results</a></li><li><a href=#lessons-learned>Lessons Learned</a></li><li><a href=#future-work>Future Work</a></li><li><a href=#code-snippets>Code Snippets</a></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><h2 class="relative group">Introduction<div id=introduction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#introduction aria-label=Anchor>#</a></span></h2><p>Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that involves identifying and classifying entities such as persons, organizations, locations, and miscellaneous names within raw text. In this project, we implemented a Transformer-based NER system using PyTorch and HuggingFace’s Transformers library, focusing on fine-tuning the DistilBERT model on the widely used CoNLL-2003 dataset.</p><p>Our objectives were threefold:
1. Build a high-performing NER model that correctly captures entity spans.
2. Explore how subword tokenization affects sequence labeling tasks.
3. Learn the practical aspects of PyTorch-based model training and evaluation in an NER setting.</p><p>This blog presents a detailed walkthrough of the model architecture, data preprocessing, training strategy, alignment logic, and evaluation techniques we employed.</p><h2 class="relative group">Dataset and Preprocessing<div id=dataset-and-preprocessing class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#dataset-and-preprocessing aria-label=Anchor>#</a></span></h2><p>The CoNLL-2003 dataset contains annotations for four entity types: PERSON, ORGANIZATION, LOCATION, and MISC, using the BIO tagging scheme. Each sentence is tokenized, and each token is assigned a BIO tag.</p><p>We used HuggingFace’s datasets library to load and process the data. After tokenization via WordPiece, one critical preprocessing step was to align word-level BIO labels with the resulting subword tokens. Only the first subword receives the label, while others are masked from loss computation.</p><h2 class="relative group">Model Architecture<div id=model-architecture class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#model-architecture aria-label=Anchor>#</a></span></h2><p>We used the distilbert-base-uncased model from HuggingFace, which is a lightweight version of BERT. It provides a good balance between speed and performance, making it suitable for fast experimentation.</p><p>We attached a linear layer on top of the Transformer to perform token-level classification:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForTokenClassification</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForTokenClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;distilbert-base-uncased&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_labels</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>label2id</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>id2label</span><span class=o>=</span><span class=n>id2label</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>label2id</span><span class=o>=</span><span class=n>label2id</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>The classification head predicts one of the BIO tags for each token.</p><p>Label Alignment Strategy</p><p>A key challenge in NER using transformers is dealing with subword tokenization. For example, the word “Washington” might be split into [“wash”, “##ington”]. We only apply the label to the first subword and mask the rest using -100 during loss computation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>align_labels_with_tokens</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span> <span class=n>word_ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>aligned_labels</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>previous_word_id</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>word_id</span> <span class=ow>in</span> <span class=n>word_ids</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>word_id</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>aligned_labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=o>-</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>word_id</span> <span class=o>!=</span> <span class=n>previous_word_id</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>aligned_labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>labels</span><span class=p>[</span><span class=n>word_id</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>aligned_labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=o>-</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>previous_word_id</span> <span class=o>=</span> <span class=n>word_id</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>aligned_labels</span>
</span></span></code></pre></div><h2 class="relative group">Training<div id=training class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#training aria-label=Anchor>#</a></span></h2><p>We used AdamW as the optimizer, combined with a linear learning rate scheduler with warm-up. The loss function is cross-entropy, and we experimented with both uniform and weighted loss to address class imbalance.</p><p>Training was conducted over multiple epochs, with evaluation performed on a validation split after each epoch.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>TrainingArguments</span><span class=p>,</span> <span class=n>Trainer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>&#34;./results&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>evaluation_strategy</span><span class=o>=</span><span class=s2>&#34;epoch&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>save_strategy</span><span class=o>=</span><span class=s2>&#34;epoch&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>5e-5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_eval_batch_size</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>logging_dir</span><span class=o>=</span><span class=s2>&#34;./logs&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>logging_steps</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h2 class="relative group">Evaluation<div id=evaluation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#evaluation aria-label=Anchor>#</a></span></h2><p>We evaluated our model using the seqeval library, which calculates entity-level precision, recall, and F1-score:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>seqeval.metrics</span> <span class=kn>import</span> <span class=n>classification_report</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>classification_report</span><span class=p>(</span><span class=n>true_labels</span><span class=p>,</span> <span class=n>predicted_labels</span><span class=p>))</span>
</span></span></code></pre></div><p>Unlike token-wise accuracy, entity-level F1 ensures we only count correct predictions when both the boundaries and the class are correct. This is crucial in applications where partial credit is not acceptable.</p><h2 class="relative group">Results<div id=results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#results aria-label=Anchor>#</a></span></h2><p>The fine-tuned DistilBERT model achieved:
• F1 Score: 91.2%
• Precision: 92.5%
• Recall: 90.0%</p><p>This performance is competitive with larger models while benefiting from faster training and inference.</p><h2 class="relative group">Lessons Learned<div id=lessons-learned class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#lessons-learned aria-label=Anchor>#</a></span></h2><p>This project revealed several nuanced aspects of building NER systems with Transformers:
• Token classification is more complex than it seems, especially due to subword issues.
• Proper alignment and loss masking is essential to avoid noisy gradients.
• Span-level metrics like F1 are the gold standard for evaluating NER.
• Lightweight models like DistilBERT are great for rapid iteration without compromising too much on accuracy.</p><h2 class="relative group">Future Work<div id=future-work class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#future-work aria-label=Anchor>#</a></span></h2><p>Given more time, we would consider:
• Adding a CRF layer to model label transitions.
• Trying larger models like RoBERTa or domain-specific BERTs.
• Applying this pipeline to domain-specific datasets (e.g., clinical notes or legal contracts).
• Exporting the model to ONNX and optimizing it for inference in production environments.</p><h2 class="relative group">Code Snippets<div id=code-snippets class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#code-snippets aria-label=Anchor>#</a></span></h2><p>Below is the key logic for loss masking and label alignment during preprocessing:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>align_labels_with_tokens</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span> <span class=n>word_ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>aligned_labels</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>previous_word_id</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>word_id</span> <span class=ow>in</span> <span class=n>word_ids</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>word_id</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>aligned_labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=o>-</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>word_id</span> <span class=o>!=</span> <span class=n>previous_word_id</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>aligned_labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>labels</span><span class=p>[</span><span class=n>word_id</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>aligned_labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=o>-</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>previous_word_id</span> <span class=o>=</span> <span class=n>word_id</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>aligned_labels</span>
</span></span></code></pre></div><p>You can find the full implementation and training notebook in the project GitHub repository.</p><p>⸻</p><p>This project served as a deep dive into token-level sequence modeling, fine-tuning transformer architectures for NER, and evaluating structured prediction models in NLP using PyTorch.</p></div></div><script type=text/javascript src=/js/page.min.407e5b2727c1f241c95d53db24c776bea71bfc18e09511b815d669ad8caca6e9e18a53864ad364b1ccccfa2f2956768d33cfe193bfb64d3406f3b5ece354ba57.js integrity="sha512-QH5bJyfB8kHJXVPbJMd2vqcb/BjglRG4FdZprYyspunhilOGStNksczM+i8pVnaNM8/hk7+2TTQG87Xs41S6Vw==" data-oid=views_projects/ner_transformer.md data-oid-likes=likes_projects/ner_transformer.md></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/projects/dea_project/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Optimizing Wind Power Plant Locations in Iran Using DEA</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-06-20T00:00:00+00:00>20 June 2025</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/projects/quasi-netwon/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Impact of Memory Size on Quasi-Newton Methods</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-06-20T00:00:00+00:00>20 June 2025</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/categories/ title=Categories>Categories</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-index-500" data-url=http://localhost:1313/><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>